# langchain-docs

### Different types of declaring models
1. You can declare model name in create_agent using **model identifier string**. Its a string that follows the format `provider:model` (e.g. openai:gpt-5)
```
from langchain.agent import create_agent  
agent= create_agent(model="ollama:llama3.1")

result= agent.invoke(
  {"messages: ["role":"user", "content", "How is weather in Frankfurt?"]
)

print(f"result is {result['messages[[-1].content)
```

2. Using provider packages  
For more control over the model configuration, initialize a model instance directly using the provider package.
Following are some of the configuration parameters

    ğŸ§  Temperature:  
    Purpose: Controls randomness / creativity of the modelâ€™s responses.  
    Temperature	Behavior       
    0	ğŸ”’ Deterministic â€” always produces the same output for the same input, 	Perfect for factual Q&A or coding tasks    
    0.7	âš–ï¸ Balanced â€” a mix of consistency and creativity, 	Good for summarization or brainstorming    
    1.0+	ğŸ¨ Highly creative / random â€” more diverse wording, but sometimes less accurate	Useful for poetry, story generation  
    
    ğŸ§® max_tokens:  
    Purpose: Defines the maximum number of tokens the model can generate in its response.  
    Think of tokens as chunks of text â€” roughly 1 token â‰ˆ 4 characters in English, or about Â¾ of a word.  
    For example:  
    â€œChatGPT is amazing!â€ â†’ ~4 tokens.  
    1000 tokens â‰ˆ 750 words.  
    1 token = 0.75 words  

    â±ï¸ timeout  
     Purpose: Sets a maximum time (in seconds) the model is allowed to take before throwing a timeout error.  
     Example: timeout=30 means if Ollama hasnâ€™t responded in 30 seconds, it raises an error instead of hanging.  
     Useful when youâ€™re dealing with slow local models (like large llama3 variants) or in web apps.   
      âœ… Tips:  
      Default is usually None (wait forever).  
      For interactive scripts, use 30â€“60 seconds.  
      For API servers, use something shorter (like 10â€“15s).  
   
     max_retries: Maximum number of retry attempts for failed requests.  
       When your code sends a request to a model API (like Ollama, OpenAI, etc.), sometimes the request fails â€” due to:  
           temporary network glitches ğŸ’»,  
           model server overload ğŸ§ ,  
           or timeout errors â±ï¸.    
     Instead of crashing immediately, the LangChain client can automatically retry the request a few times before giving up.       
     Thatâ€™s what max_retries controls â€” how many times it will retry.  
   
   base_url: Custom API endpoint URL. It tells LangChain where to send the API requests â€” i.e., the endpoint where your model is running and can be accessed.        
  
   rate_limiter: A BaseRateLimiter instance to control request rate.  
   its an an optional parameter that lets you control how often your code sends requests to an API (or local model like Ollama).  
   It accepts a BaseRateLimiter object â€” a built-in LangChain utility that keeps your requests under a certain rate (like â€œno more than 5 requests per secondâ€).  
   This is critical when:  
     Youâ€™re hitting APIs that have usage limits (e.g., OpenAI, Anthropic, Hugging Face, etc.), or  
     You want to avoid overloading your local model or server.  
   
   Putting it all togther  
```
from langchain_core.rate_limiters import InMemoryRateLimiter
from langchain_ollama import ChatOllama  # use model provider package

# Allow max 5 requests per second
rate_limiter = InMemoryRateLimiter(requests_per_second=5)

model = ChatOllama(
    model="llama3.1",
    temperature=0,      # deterministic, precise
    max_tokens=1000,    # allow up to ~750 words
    timeout=30,          # stop if it takes longer than 30s
    base_url="http://localhost:11434",  # Default Ollama endpoint
    max_retries=3 , # <-- Will retry up to 3 times on failure
    rate_limiter=rate_limiter

)
```  
3. Init_chat model  
init_chat_model()  is the **recommended approach** for initializing models in LangChain.  
Why it's recommended:  
Unified interface - Works across different providers without changing code  
Flexibility - Easy to switch between providers (OpenAI, Anthropic, Azure, etc.)  
Simpler syntax - Less boilerplate than direct instantiation  
```
from langchain.chat_models import init_chat_model
model= init_chat_model("gpt-4.1")

response = model.invoke("Why do parrots talk?")
```

